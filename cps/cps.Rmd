---
output:
  md_document
---

## Hourly wages in the Current Population Survey

Learning goal:  
- observe how and why collinearity affects the estimated coefficient and the ANOVA table for a multiple regression model.  


Data files:  
- [cps.csv](http://jgscott.github.io/teaching/data/cps.csv): data from the [Current Population Survey](http://www.census.gov/cps/), a major source of data about the American labor force.


### ANOVA tables in multiple regression  

We'll start by loading the mosaic library, reading in the CPS data set, and summarizing the variables.
```{r, message=FALSE}
library(mosaic)
cps = read.csv("cps.csv", header=TRUE)  # Or use Import Dataset
summary(cps)
```

There are 11 variables in this data set:  
- wage: a person's hourly wage in dollars (the data is from 1985).  
- educ: number of years of formal education.  Here 12 indicates the completion of high school.  
- race: white or non-white.      
- sex: male or female.  
- hispanic: an indicator of whether the person is Hispanic or non-Hispanic.  
- south: does the person live in a southern (S) or non-southern (NS) state?  
- married: is the person married or single?  
- exper: number of years of work experience  
- union: an indicator for whether the person is in a union or not.  
- age: age in years  
- sector: clerical, construction, management, manufacturing, professional (lawyer/doctor/accountant/etc), sales, service, or other.


First consider a two-variable regression model that uses a person's education level and sector of employment as predictors of his or her wage:
```{r}
lm1 = lm(wage ~ educ + sector, data=cps)
summary(lm1)
```

Now see what happens when we switch the order of the two variables:
```{r}
lm2 = lm(wage ~ sector + educ, data=cps)
summary(lm2)
```

In a word, nothing!  The coefficients, standard errors, t statistics, and p-values are all the same.  That's because the model itself---that is, the underlying regression equation relating the outcome to the predictors---is the same regardless of the order in which we name the variables.  (That's because we add the individual terms in the regression equation together, and [addition is commutative](http://en.wikipedia.org/wiki/Commutative_property).)  This is comforting: it means our model doesn't depend on some arbitrary choice of how to order the variables.

However, the ANOVA tables for the two models are different.  Let's use the `simple_anova` function from my website:
```{r}
source('http://jgscott.github.io/teaching/r/utils/class_utils.R')
```

Now we'll use this to construct an ANOVA table.  In the first table, it looks like education contributes more to the fit of the model than sector of employment:
```{r}
simple_anova(lm1)
```

In the second table, it now looks like a person's sector of employment contributes more to the fit than his or her education:
```{r}
simple_anova(lm2)
```

In other words, the ANOVA table usually *does* depend on the order in which we name the variables, even though the model itself does not.  The only exception is when the variables are independent of one another.  This exception doesn't apply here, because some sectors of the economy have more educated workers than other sectors.  Said concisely, the two variables are correlated (collinear) with each other:  
```{r}
bwplot(educ ~ sector, data=cps)
```

We therefore reach an important conclusion about the ANOVA table for a multiple-regression model:   
- The ANOVA table attempts to partition credit among the variables by measuring their contribution to the model's predictable sums of squares.   More specifically, it assigns credit by adding the variables one at a time and measuring the corresponding decrease in the residual sum of squares.   
- But the table depends on the ordering of the variables, and the ordering of the variables is arbitrary.  
-  We therefore cannot give credit to the individual variables in a model without making an arbitrary decision about their order.  

Though this seems like a paradox, it's really a manifestation of a broader concept.  In a regression model, the variables work as a team.  And it is difficult to partition credit to the individuals who compose a team---whether it's a team of lawyers, film-makers, or basketball players---except in the rare case where the individuals contribute to the team in totally independent ways.  


### Adding collinear variables to a model   

In the presence of collinearity, adding new variables to a baseline model will change the coefficients of the old variables.  That's because:  
- a coefficient in a multiple regression model is a _partial relationship_ between the predictor and the response, holding other variables constant  
- a partial relationship depends on context, i.e. on what variables are being held constant.  
- changing the context (by adding new variables) will therefore change the partial relationship.  


Let's see this principle in action, starting with a baseline model of wages versus a sector of employment, education level, and whether a worker is a member of union.  
```{r}
lm3 = lm(wage ~ sector + educ + union, data=cps)
summary(lm3)
```

Look at the coefficient on the union dummy variable: it looks like the wage premium for union members is about $2.20 per hour, holding education and employment sector constant.  

But look at what happens when we add age to the model:  
```{r}
lm4 = lm(wage ~ sector + educ + union + age, data=cps)
summary(lm4)
```

Now our estimate of the wage premium for union members is a lot less: about \$1.80 per hour rather than \$2.20.  What happened?

Well, union members tend to be older than non-union members (i.e. union status and age are correlated/collinear):  
```{r}
mean(age ~ union, data=cps)
```

And older workers tend to earn more on average, than younger workers (the correlation is about 17%):  
```{r}
cor(wage ~ age, data=cps)
```

That makes age a confounding variable for the relationship between union status and wages.  Controlling for this confounding variable changes our estimate of the wage premium for union membership: some (but not all) of the variation in wages that we previously were attributing to union membership now gets attributed to age, instead.  

This is a very general phenomenon in multiple regression modeling.  If you add a variable to a baseline model, and that variable is correlated with some of the variables already in the model, then the coefficients on those old variables will change in the new model.   